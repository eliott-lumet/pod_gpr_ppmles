{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POD--GPR surrogate modeling example\n",
    "\n",
    "This notebook presents how to build and validate a POD--GPR [1] surrogate model\n",
    "of microscale pollutant dispersion. This surrogate learns the dependence of the\n",
    "3-D mean concentration field on meteorological forcing (the inlet wind \n",
    "direction and friction velocity) based on a dataset of precomputed LES called \n",
    "PPMLES. It can be used to significantly accelerate dispersion predictions and \n",
    "for applications that require large ensemble of model evaluations, such as data\n",
    "assimilation [2].\n",
    "\n",
    "The PPMLES dataset will soon be available online at Zenodo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, Matern, ConstantKernel\n",
    "\n",
    "from auxiliaries import min_max_normalize, LogCenteredScaler, MultiOutputRegressor\n",
    "from metrics import fb, nmse, fac2, mg, vg, fms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean to train the POD-GPR model only once\n",
    "first_run = True\n",
    "\n",
    "# POD-GPR parameters\n",
    "n_modes = 10      # Number of POD modes (must be < n_train)\n",
    "c_log_cut = 1e-4  # Threshold for log transformation\n",
    "\n",
    "# GP hyperparameter\n",
    "nu = 2.5          # Matern kernel order\n",
    "\n",
    "#### Train set and test set definitions\n",
    "# WARNING: PPMLES data are uniformly mixed into two feature classes (samples {0:100} and {100:200})\n",
    "train_indices = np.array([k for k in range(80)] + [k for k in range(100, 180)])\n",
    "test_indices = np.array([k for k in range(80, 100)] + [k for k in range(180, 200)])\n",
    "n_test = np.shape(test_indices)[0]\n",
    "\n",
    "### Load the set of time-averaged concentration fields predicted by LES\n",
    "fieldsf = h5py.File('data/ave_fields.h5', 'r')\n",
    "fields_train, fields_test = fieldsf['c'][train_indices], fieldsf['c'][test_indices]\n",
    "\n",
    "# Fields preprocessing\n",
    "fields_train[fields_train<0] = 0\n",
    "fields_test[fields_test<0] = 0\n",
    "\n",
    "### Load the set of input_parameters\n",
    "inputf = h5py.File('data/input_parameters.h5', 'r')\n",
    "alpha_train, alpha_test = inputf['alpha_inlet'][train_indices], inputf['alpha_inlet'][test_indices]\n",
    "ustar_train, ustar_test = inputf['friction_velocity'][train_indices], inputf['friction_velocity'][test_indices]\n",
    "n_inputs = 2\n",
    "\n",
    "# Input parameter variation ranges\n",
    "ALPHA_MIN, ALPHA_MAX = -90., 30.\n",
    "USTAR_MIN, USTAR_MAX = 0.0740, 0.8875\n",
    "\n",
    "### Load the mesh to get the dual volumes of each node\n",
    "meshf = h5py.File('data/mesh.h5')\n",
    "node_volumes = meshf['Nodes']['volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POD basis computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the snapshots for the POD\n",
    "fields_scaler = LogCenteredScaler(c_log_cut)\n",
    "fields_scaler.fit(fields_train, node_volumes)\n",
    "rescaled_fields_train = fields_scaler.transform(fields_train)\n",
    "\n",
    "# Build the POD basis\n",
    "pod = PCA(n_components=n_modes, whiten=True, random_state=0)\n",
    "pod = pod.fit(rescaled_fields_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Gaussian Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scales the inputs\n",
    "alpha_train = min_max_normalize(alpha_train, ALPHA_MIN, ALPHA_MAX)\n",
    "ustar_train = min_max_normalize(ustar_train, USTAR_MIN, USTAR_MAX)\n",
    "inputs_train = np.array((alpha_train, ustar_train)).T\n",
    "\n",
    "# Compute the POD coeffs of the training samples\n",
    "pod_coeffs_train = pod.transform(rescaled_fields_train)\n",
    "\n",
    "# Define the GPs\n",
    "kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=[0.1, 50]) \\\n",
    "                        * Matern(np.ones(n_inputs), length_scale_bounds=[1e-3, 50], nu=nu) \\\n",
    "                        + WhiteKernel(noise_level=1e-1, noise_level_bounds=[1e-5,1])\n",
    "multi_gpr = MultiOutputRegressor(GaussianProcessRegressor(kernel, \n",
    "                                                          n_restarts_optimizer=15,\n",
    "                                                          random_state = 0), n_jobs=10)\n",
    "multi_gpr = multi_gpr.fit(inputs_train, pod_coeffs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POD-GPR validation on the independent test set\n",
    "The POD-GPR accuracy is evaluated using the standard air quality metrics defined by [3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics array initialization\n",
    "fb_list = np.zeros(n_test)\n",
    "nmse_list = np.zeros(n_test)\n",
    "fac2_list = np.zeros(n_test)\n",
    "mg_list = np.zeros(n_test)\n",
    "vg_list = np.zeros(n_test)\n",
    "fms_1ppm_list = np.zeros(n_test)\n",
    "fms_0_01ppm_list = np.zeros(n_test)\n",
    "\n",
    "### POD-GPR predictions\n",
    "tstart = time.time()\n",
    "normalized_parameters = np.array([min_max_normalize(alpha_test, ALPHA_MIN, ALPHA_MAX), \n",
    "                                  min_max_normalize(ustar_test, USTAR_MIN, USTAR_MAX)]).T\n",
    "gpr_estimates = multi_gpr.predict(normalized_parameters, return_std=False)\n",
    "fields_test_pod_gpr = fields_scaler.inverse_transform(pod.inverse_transform(gpr_estimates))\n",
    "integration_time = (time.time() - tstart)/n_test\n",
    "\n",
    "### Air quality metrics computation\n",
    "for i in range(n_test):\n",
    "    fb_list[i] = fb(fields_test_pod_gpr[i], fields_test[i], weights=node_volumes)\n",
    "    nmse_list[i] = nmse(fields_test_pod_gpr[i], fields_test[i], weights=node_volumes)\n",
    "    fac2_list[i] = fac2(fields_test_pod_gpr[i], fields_test[i], c_log_cut, weights=node_volumes)\n",
    "    mg_list[i] = mg(fields_test_pod_gpr[i], fields_test[i], c_log_cut, weights=node_volumes)\n",
    "    vg_list[i] = vg(fields_test_pod_gpr[i], fields_test[i], c_log_cut, weights=node_volumes)\n",
    "    fms_1ppm_list[i] = fms(fields_test_pod_gpr[i], fields_test[i], 1., weights=node_volumes)\n",
    "    fms_0_01ppm_list[i] = fms(fields_test_pod_gpr[i], fields_test[i], 0.01, weights=node_volumes)\n",
    "\n",
    "### Print metric scores averaged over the test set\n",
    "print(f\"\\n=============================================================================\\n\\t Air quality scores averaged over all the test samples\\n=============================================================================\\n\")\n",
    "print(f\"Ensemble averaged FB for the mean concentration = {np.mean(fb_list):.2f}\")\n",
    "print(f\"Ensemble averaged NMSE for the mean concentration = {np.mean(nmse_list):.2f}\")\n",
    "print(f\"Ensemble averaged FAC2 for the mean concentration = {np.mean(fac2_list):.2f}\")\n",
    "print(f\"Ensemble averaged MG for the mean concentration = {np.mean(mg_list):.2f}\")\n",
    "print(f\"Ensemble averaged VG for the mean concentration = {np.mean(vg_list):.2f}\")\n",
    "print(f\"Ensemble averaged FMS for iso=1-ppm = {np.mean(fms_1ppm_list):.2f}\")\n",
    "print(f\"Ensemble averaged FMS for iso=0.01-ppm = {np.mean(fms_0_01ppm_list):.2f}\")\n",
    "print(f\"Ensemble averaged POD-GPR prediction time = {integration_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "[1] Marrel, A., Perot, N., and Mottet, C. (2015). Development of a surrogate model and sensitivity analysis for spatio-temporal numerical simulators. Stochastic Environmental Research and Risk Assessment, 29(3):959–974. ISSN 1436-3259. DOI: https://doi.org/10.1007/s00477-014-0927-y.\n",
    "\n",
    "[2] Lumet, E. (2024). Assessing and reducing uncertainty in large-eddy simulation for microscale atmospheric dispersion. PhD thesis, Université Toulouse III - Paul Sabatier. URL: https://theses.fr/2024TLSES003. Accessed: 2024-07-08.\n",
    "\n",
    "[3] Chang, J. and Hanna, S. (2004). Air quality model performance evaluation. Meteorol. Atm. Phys, 87(1):167–196. DOI: https://doi.org/10.1007/s00703-003-0070-7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-jupyter-eliott",
   "language": "python",
   "name": "python-jupyter-eliott"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
